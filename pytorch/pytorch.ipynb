{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.555621500Z",
     "start_time": "2024-02-14T21:46:46.548082800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.618428500Z",
     "start_time": "2024-02-14T21:46:46.551630200Z"
    }
   },
   "id": "70c65c347689c6e",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    \n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.619425900Z",
     "start_time": "2024-02-14T21:46:46.589548100Z"
    }
   },
   "id": "8ce306268ed9b0d1",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.653633900Z",
     "start_time": "2024-02-14T21:46:46.612444700Z"
    }
   },
   "id": "1a83d699c064918d",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.654630900Z",
     "start_time": "2024-02-14T21:46:46.622928500Z"
    }
   },
   "id": "9c2ef7c74b362584",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.655627800Z",
     "start_time": "2024-02-14T21:46:46.629392Z"
    }
   },
   "id": "7f1f3a3363ffee86",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:46:46.689434200Z",
     "start_time": "2024-02-14T21:46:46.640692600Z"
    }
   },
   "id": "5f730ea9a931f1cd",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310104  [   64/60000]\n",
      "loss: 2.282546  [ 6464/60000]\n",
      "loss: 2.271273  [12864/60000]\n",
      "loss: 2.267850  [19264/60000]\n",
      "loss: 2.243557  [25664/60000]\n",
      "loss: 2.218509  [32064/60000]\n",
      "loss: 2.238286  [38464/60000]\n",
      "loss: 2.203471  [44864/60000]\n",
      "loss: 2.204998  [51264/60000]\n",
      "loss: 2.168993  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 2.160171 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.175207  [   64/60000]\n",
      "loss: 2.157259  [ 6464/60000]\n",
      "loss: 2.101014  [12864/60000]\n",
      "loss: 2.124861  [19264/60000]\n",
      "loss: 2.072226  [25664/60000]\n",
      "loss: 2.007615  [32064/60000]\n",
      "loss: 2.054337  [38464/60000]\n",
      "loss: 1.969816  [44864/60000]\n",
      "loss: 1.981094  [51264/60000]\n",
      "loss: 1.906002  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.899131 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.931684  [   64/60000]\n",
      "loss: 1.896624  [ 6464/60000]\n",
      "loss: 1.776191  [12864/60000]\n",
      "loss: 1.831017  [19264/60000]\n",
      "loss: 1.721192  [25664/60000]\n",
      "loss: 1.659628  [32064/60000]\n",
      "loss: 1.700340  [38464/60000]\n",
      "loss: 1.593678  [44864/60000]\n",
      "loss: 1.618422  [51264/60000]\n",
      "loss: 1.515610  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.529171 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.590003  [   64/60000]\n",
      "loss: 1.556340  [ 6464/60000]\n",
      "loss: 1.402313  [12864/60000]\n",
      "loss: 1.485548  [19264/60000]\n",
      "loss: 1.367676  [25664/60000]\n",
      "loss: 1.353929  [32064/60000]\n",
      "loss: 1.376988  [38464/60000]\n",
      "loss: 1.300721  [44864/60000]\n",
      "loss: 1.330012  [51264/60000]\n",
      "loss: 1.234362  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.257628 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.327679  [   64/60000]\n",
      "loss: 1.313992  [ 6464/60000]\n",
      "loss: 1.143535  [12864/60000]\n",
      "loss: 1.259177  [19264/60000]\n",
      "loss: 1.133541  [25664/60000]\n",
      "loss: 1.152133  [32064/60000]\n",
      "loss: 1.178751  [38464/60000]\n",
      "loss: 1.117753  [44864/60000]\n",
      "loss: 1.151608  [51264/60000]\n",
      "loss: 1.071267  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.089096 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:47:26.876468100Z",
     "start_time": "2024-02-14T21:46:46.649617200Z"
    }
   },
   "id": "b1112dc2fcb22928",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:47:26.886397900Z",
     "start_time": "2024-02-14T21:47:26.871481600Z"
    }
   },
   "id": "9931515ad1556f64",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:47:26.926428Z",
     "start_time": "2024-02-14T21:47:26.881411600Z"
    }
   },
   "id": "a96a164c015c6c8e",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:47:26.948084800Z",
     "start_time": "2024-02-14T21:47:26.927425600Z"
    }
   },
   "id": "2985f3507871b56b",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(pred(test_data[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T21:50:05.937306500Z",
     "start_time": "2024-02-14T21:50:05.897053Z"
    }
   },
   "id": "f76d06a479061c61",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30309770.0\n",
      "1 24341560.0\n",
      "2 20327402.0\n",
      "3 16258935.0\n",
      "4 12074539.0\n",
      "5 8379966.5\n",
      "6 5574837.0\n",
      "7 3677681.75\n",
      "8 2473084.0\n",
      "9 1727488.5\n",
      "10 1261133.125\n",
      "11 960544.5\n",
      "12 758384.625\n",
      "13 615936.375\n",
      "14 510782.65625\n",
      "15 430158.8125\n",
      "16 366478.84375\n",
      "17 314924.28125\n",
      "18 272537.03125\n",
      "19 237045.609375\n",
      "20 207144.0625\n",
      "21 181760.046875\n",
      "22 160105.09375\n",
      "23 141492.734375\n",
      "24 125431.234375\n",
      "25 111496.5234375\n",
      "26 99360.5546875\n",
      "27 88770.8125\n",
      "28 79507.78125\n",
      "29 71365.609375\n",
      "30 64176.046875\n",
      "31 57821.140625\n",
      "32 52204.6640625\n",
      "33 47220.44140625\n",
      "34 42789.2421875\n",
      "35 38835.9453125\n",
      "36 35297.99609375\n",
      "37 32127.04296875\n",
      "38 29287.09765625\n",
      "39 26736.521484375\n",
      "40 24440.6640625\n",
      "41 22367.63671875\n",
      "42 20497.033203125\n",
      "43 18805.08203125\n",
      "44 17272.810546875\n",
      "45 15882.33203125\n",
      "46 14619.517578125\n",
      "47 13470.86328125\n",
      "48 12424.666015625\n",
      "49 11470.552734375\n",
      "50 10599.4453125\n",
      "51 9804.076171875\n",
      "52 9076.8623046875\n",
      "53 8411.34375\n",
      "54 7800.8974609375\n",
      "55 7241.3662109375\n",
      "56 6726.66015625\n",
      "57 6254.146484375\n",
      "58 5819.30859375\n",
      "59 5418.6904296875\n",
      "60 5049.0771484375\n",
      "61 4707.9150390625\n",
      "62 4392.5859375\n",
      "63 4101.0947265625\n",
      "64 3831.4306640625\n",
      "65 3581.74560546875\n",
      "66 3350.3291015625\n",
      "67 3135.699462890625\n",
      "68 2936.609130859375\n",
      "69 2751.612060546875\n",
      "70 2579.621337890625\n",
      "71 2419.7265625\n",
      "72 2270.86572265625\n",
      "73 2132.23681640625\n",
      "74 2003.14453125\n",
      "75 1882.7669677734375\n",
      "76 1770.426513671875\n",
      "77 1665.543701171875\n",
      "78 1567.53173828125\n",
      "79 1475.89599609375\n",
      "80 1390.19873046875\n",
      "81 1310.032958984375\n",
      "82 1234.96630859375\n",
      "83 1164.65673828125\n",
      "84 1098.7637939453125\n",
      "85 1037.0133056640625\n",
      "86 979.11083984375\n",
      "87 924.7838134765625\n",
      "88 873.747314453125\n",
      "89 825.8109130859375\n",
      "90 780.7617797851562\n",
      "91 738.4130249023438\n",
      "92 698.5682373046875\n",
      "93 661.0741577148438\n",
      "94 625.7813720703125\n",
      "95 592.5543212890625\n",
      "96 561.26318359375\n",
      "97 531.783447265625\n",
      "98 503.9585876464844\n",
      "99 477.7220458984375\n",
      "100 452.96453857421875\n",
      "101 429.6012268066406\n",
      "102 407.54345703125\n",
      "103 386.71453857421875\n",
      "104 367.0403137207031\n",
      "105 348.44305419921875\n",
      "106 330.8634948730469\n",
      "107 314.2415466308594\n",
      "108 298.514404296875\n",
      "109 283.63677978515625\n",
      "110 269.55322265625\n",
      "111 256.22265625\n",
      "112 243.60569763183594\n",
      "113 231.6537628173828\n",
      "114 220.3253936767578\n",
      "115 209.5913543701172\n",
      "116 199.41612243652344\n",
      "117 189.76943969726562\n",
      "118 180.62217712402344\n",
      "119 171.94137573242188\n",
      "120 163.7071990966797\n",
      "121 155.8932647705078\n",
      "122 148.4705352783203\n",
      "123 141.4236602783203\n",
      "124 134.7323455810547\n",
      "125 128.37437438964844\n",
      "126 122.33441925048828\n",
      "127 116.59552001953125\n",
      "128 111.1432876586914\n",
      "129 105.956298828125\n",
      "130 101.02525329589844\n",
      "131 96.33587646484375\n",
      "132 91.87483215332031\n",
      "133 87.63150024414062\n",
      "134 83.59333801269531\n",
      "135 79.752685546875\n",
      "136 76.09658813476562\n",
      "137 72.61514282226562\n",
      "138 69.3004150390625\n",
      "139 66.14422607421875\n",
      "140 63.1378288269043\n",
      "141 60.27395248413086\n",
      "142 57.546302795410156\n",
      "143 54.94756317138672\n",
      "144 52.47074890136719\n",
      "145 50.11043930053711\n",
      "146 47.86016845703125\n",
      "147 45.71553421020508\n",
      "148 43.67036819458008\n",
      "149 41.72040939331055\n",
      "150 39.861446380615234\n",
      "151 38.08748245239258\n",
      "152 36.39487838745117\n",
      "153 34.780460357666016\n",
      "154 33.24021911621094\n",
      "155 31.771068572998047\n",
      "156 30.368526458740234\n",
      "157 29.030258178710938\n",
      "158 27.753068923950195\n",
      "159 26.53319549560547\n",
      "160 25.368728637695312\n",
      "161 24.257198333740234\n",
      "162 23.195615768432617\n",
      "163 22.18158721923828\n",
      "164 21.21404266357422\n",
      "165 20.289363861083984\n",
      "166 19.405925750732422\n",
      "167 18.562118530273438\n",
      "168 17.75631332397461\n",
      "169 16.985836029052734\n",
      "170 16.25031089782715\n",
      "171 15.547090530395508\n",
      "172 14.875274658203125\n",
      "173 14.232880592346191\n",
      "174 13.618942260742188\n",
      "175 13.032094955444336\n",
      "176 12.471181869506836\n",
      "177 11.935005187988281\n",
      "178 11.422357559204102\n",
      "179 10.932204246520996\n",
      "180 10.463531494140625\n",
      "181 10.015156745910645\n",
      "182 9.586790084838867\n",
      "183 9.176947593688965\n",
      "184 8.78500747680664\n",
      "185 8.41006851196289\n",
      "186 8.051183700561523\n",
      "187 7.70826530456543\n",
      "188 7.380160331726074\n",
      "189 7.066313743591309\n",
      "190 6.766018867492676\n",
      "191 6.478560447692871\n",
      "192 6.2036237716674805\n",
      "193 5.940517425537109\n",
      "194 5.6887359619140625\n",
      "195 5.447981834411621\n",
      "196 5.217328071594238\n",
      "197 4.996677875518799\n",
      "198 4.785587310791016\n",
      "199 4.583465099334717\n",
      "200 4.390114784240723\n",
      "201 4.204771041870117\n",
      "202 4.027653694152832\n",
      "203 3.857954978942871\n",
      "204 3.6955366134643555\n",
      "205 3.540008306503296\n",
      "206 3.3911406993865967\n",
      "207 3.248762369155884\n",
      "208 3.112255573272705\n",
      "209 2.981654644012451\n",
      "210 2.8564844131469727\n",
      "211 2.7366349697113037\n",
      "212 2.6219801902770996\n",
      "213 2.5122056007385254\n",
      "214 2.4069948196411133\n",
      "215 2.3062803745269775\n",
      "216 2.2098379135131836\n",
      "217 2.117419719696045\n",
      "218 2.0289223194122314\n",
      "219 1.9441907405853271\n",
      "220 1.8629789352416992\n",
      "221 1.7851608991622925\n",
      "222 1.7107880115509033\n",
      "223 1.6394429206848145\n",
      "224 1.571017861366272\n",
      "225 1.5056657791137695\n",
      "226 1.442972183227539\n",
      "227 1.3828706741333008\n",
      "228 1.3253164291381836\n",
      "229 1.2702550888061523\n",
      "230 1.2174503803253174\n",
      "231 1.1668394804000854\n",
      "232 1.1183265447616577\n",
      "233 1.0719130039215088\n",
      "234 1.0273175239562988\n",
      "235 0.9847162961959839\n",
      "236 0.9438437223434448\n",
      "237 0.9047383069992065\n",
      "238 0.8672849535942078\n",
      "239 0.8312678337097168\n",
      "240 0.7969231605529785\n",
      "241 0.7638309001922607\n",
      "242 0.7322674989700317\n",
      "243 0.7019423246383667\n",
      "244 0.6728784441947937\n",
      "245 0.6450488567352295\n",
      "246 0.6183483004570007\n",
      "247 0.5927965641021729\n",
      "248 0.5683433413505554\n",
      "249 0.5447978973388672\n",
      "250 0.5223042964935303\n",
      "251 0.5007480382919312\n",
      "252 0.48008254170417786\n",
      "253 0.46027517318725586\n",
      "254 0.44123291969299316\n",
      "255 0.4230564534664154\n",
      "256 0.40563488006591797\n",
      "257 0.3888619542121887\n",
      "258 0.3728224039077759\n",
      "259 0.35749512910842896\n",
      "260 0.3427532911300659\n",
      "261 0.3286275863647461\n",
      "262 0.31509602069854736\n",
      "263 0.30213695764541626\n",
      "264 0.2897075414657593\n",
      "265 0.2777547240257263\n",
      "266 0.266360878944397\n",
      "267 0.25539150834083557\n",
      "268 0.2448793351650238\n",
      "269 0.23479832708835602\n",
      "270 0.22515641152858734\n",
      "271 0.21589887142181396\n",
      "272 0.2070349156856537\n",
      "273 0.1985187530517578\n",
      "274 0.19037285447120667\n",
      "275 0.18254849314689636\n",
      "276 0.17505450546741486\n",
      "277 0.16788548231124878\n",
      "278 0.1610029935836792\n",
      "279 0.15439030528068542\n",
      "280 0.14806707203388214\n",
      "281 0.1419840008020401\n",
      "282 0.13616609573364258\n",
      "283 0.13057005405426025\n",
      "284 0.12524324655532837\n",
      "285 0.12010139226913452\n",
      "286 0.11519479751586914\n",
      "287 0.11047288030385971\n",
      "288 0.10595732927322388\n",
      "289 0.10162318497896194\n",
      "290 0.0974372997879982\n",
      "291 0.09348072111606598\n",
      "292 0.08964472264051437\n",
      "293 0.08598683029413223\n",
      "294 0.08246512711048126\n",
      "295 0.07908959686756134\n",
      "296 0.07586434483528137\n",
      "297 0.07277211546897888\n",
      "298 0.06979496777057648\n",
      "299 0.06695567071437836\n",
      "300 0.06421871483325958\n",
      "301 0.061600904911756516\n",
      "302 0.05908060073852539\n",
      "303 0.056685857474803925\n",
      "304 0.05436968058347702\n",
      "305 0.05216329172253609\n",
      "306 0.05003288388252258\n",
      "307 0.0479947030544281\n",
      "308 0.046032171696424484\n",
      "309 0.04416458681225777\n",
      "310 0.04236431047320366\n",
      "311 0.040642380714416504\n",
      "312 0.038987159729003906\n",
      "313 0.03741531819105148\n",
      "314 0.0358877032995224\n",
      "315 0.03442143648862839\n",
      "316 0.03303210809826851\n",
      "317 0.0316934734582901\n",
      "318 0.030410079285502434\n",
      "319 0.029169689863920212\n",
      "320 0.027994748204946518\n",
      "321 0.02685067243874073\n",
      "322 0.025769619271159172\n",
      "323 0.024734823033213615\n",
      "324 0.023735344409942627\n",
      "325 0.022776750847697258\n",
      "326 0.0218600295484066\n",
      "327 0.020973168313503265\n",
      "328 0.02013065293431282\n",
      "329 0.019317414611577988\n",
      "330 0.018537230789661407\n",
      "331 0.017795536667108536\n",
      "332 0.0170806385576725\n",
      "333 0.016400126740336418\n",
      "334 0.015744898468255997\n",
      "335 0.015117567032575607\n",
      "336 0.01450386643409729\n",
      "337 0.013929382897913456\n",
      "338 0.013378196395933628\n",
      "339 0.012839117087423801\n",
      "340 0.01233135350048542\n",
      "341 0.011847736313939095\n",
      "342 0.01137198880314827\n",
      "343 0.010925140231847763\n",
      "344 0.010489187203347683\n",
      "345 0.010074838064610958\n",
      "346 0.009678617119789124\n",
      "347 0.009298257529735565\n",
      "348 0.00893035251647234\n",
      "349 0.008576307445764542\n",
      "350 0.00824148952960968\n",
      "351 0.007920405827462673\n",
      "352 0.007611039560288191\n",
      "353 0.00731842452660203\n",
      "354 0.007034647278487682\n",
      "355 0.006762524135410786\n",
      "356 0.006499307230114937\n",
      "357 0.006249044556170702\n",
      "358 0.006012211553752422\n",
      "359 0.0057776207104325294\n",
      "360 0.005554244853556156\n",
      "361 0.005340788513422012\n",
      "362 0.005138163920491934\n",
      "363 0.004941742867231369\n",
      "364 0.004751848056912422\n",
      "365 0.004573024809360504\n",
      "366 0.00440236134454608\n",
      "367 0.004238884896039963\n",
      "368 0.004078357946127653\n",
      "369 0.003924943972378969\n",
      "370 0.003777590813115239\n",
      "371 0.003637148067355156\n",
      "372 0.0035042683593928814\n",
      "373 0.0033750038128346205\n",
      "374 0.003250626614317298\n",
      "375 0.0031305523589253426\n",
      "376 0.0030173773411661386\n",
      "377 0.0029052956961095333\n",
      "378 0.0028005673084408045\n",
      "379 0.002701742108911276\n",
      "380 0.0026050880551338196\n",
      "381 0.0025117509067058563\n",
      "382 0.0024226000532507896\n",
      "383 0.0023386040702462196\n",
      "384 0.002254426246508956\n",
      "385 0.0021751499734818935\n",
      "386 0.0020967135205864906\n",
      "387 0.0020263532642275095\n",
      "388 0.0019568135030567646\n",
      "389 0.001888878527097404\n",
      "390 0.001823269296437502\n",
      "391 0.0017609031638130546\n",
      "392 0.001701398752629757\n",
      "393 0.001644147327169776\n",
      "394 0.001587670878507197\n",
      "395 0.001536066527478397\n",
      "396 0.0014856645138934255\n",
      "397 0.0014362847432494164\n",
      "398 0.0013881523627787828\n",
      "399 0.0013427063822746277\n",
      "400 0.0012996317818760872\n",
      "401 0.0012566146906465292\n",
      "402 0.001215941971167922\n",
      "403 0.0011772820726037025\n",
      "404 0.0011393064633011818\n",
      "405 0.001103177317418158\n",
      "406 0.0010690410854294896\n",
      "407 0.0010356104467064142\n",
      "408 0.0010037098545581102\n",
      "409 0.0009720397647470236\n",
      "410 0.0009430381469428539\n",
      "411 0.0009142255294136703\n",
      "412 0.0008865772979333997\n",
      "413 0.0008596560219302773\n",
      "414 0.0008347621187567711\n",
      "415 0.0008095549419522285\n",
      "416 0.0007862615748308599\n",
      "417 0.0007637396920472383\n",
      "418 0.0007405572105199099\n",
      "419 0.0007187541923485696\n",
      "420 0.000698064744938165\n",
      "421 0.0006780409021303058\n",
      "422 0.0006588392425328493\n",
      "423 0.0006408259505406022\n",
      "424 0.0006221983931027353\n",
      "425 0.00060446368297562\n",
      "426 0.0005881601828150451\n",
      "427 0.000571897835470736\n",
      "428 0.0005562343285419047\n",
      "429 0.0005415615160018206\n",
      "430 0.0005277139716781676\n",
      "431 0.0005135758547112346\n",
      "432 0.0004999880329705775\n",
      "433 0.0004869158146902919\n",
      "434 0.00047309481306001544\n",
      "435 0.0004604763234965503\n",
      "436 0.00044828158570453525\n",
      "437 0.0004367446817923337\n",
      "438 0.0004263147711753845\n",
      "439 0.00041522271931171417\n",
      "440 0.000404295336920768\n",
      "441 0.0003949659876525402\n",
      "442 0.0003842563310172409\n",
      "443 0.00037426454946398735\n",
      "444 0.00036595691926777363\n",
      "445 0.00035680827568285167\n",
      "446 0.0003486575442366302\n",
      "447 0.00033996329875662923\n",
      "448 0.0003315351204946637\n",
      "449 0.0003246221167501062\n",
      "450 0.00031650258461013436\n",
      "451 0.00030937843257561326\n",
      "452 0.00030182255432009697\n",
      "453 0.0002942208375316113\n",
      "454 0.00028772742371074855\n",
      "455 0.0002812835737131536\n",
      "456 0.00027492461958900094\n",
      "457 0.000268579984549433\n",
      "458 0.00026289059314876795\n",
      "459 0.00025685515720397234\n",
      "460 0.0002515334344934672\n",
      "461 0.00024591569672338665\n",
      "462 0.0002404467377346009\n",
      "463 0.00023480824893340468\n",
      "464 0.00022991272271610796\n",
      "465 0.0002252922859042883\n",
      "466 0.00022050325060263276\n",
      "467 0.00021567486692219973\n",
      "468 0.00021171214757487178\n",
      "469 0.00020698842126876116\n",
      "470 0.0002025779540417716\n",
      "471 0.00019827531650662422\n",
      "472 0.00019407326180953532\n",
      "473 0.00019007940136361867\n",
      "474 0.00018661326612345874\n",
      "475 0.00018272567831445485\n",
      "476 0.0001791832037270069\n",
      "477 0.0001758271100698039\n",
      "478 0.0001715521066216752\n",
      "479 0.00016885643708519638\n",
      "480 0.0001660028356127441\n",
      "481 0.0001627814199309796\n",
      "482 0.00015974744746927172\n",
      "483 0.00015643076039850712\n",
      "484 0.00015401741256937385\n",
      "485 0.0001512312301201746\n",
      "486 0.00014804358943365514\n",
      "487 0.00014571477368008345\n",
      "488 0.0001431879063602537\n",
      "489 0.00014027472934685647\n",
      "490 0.00013771824887953699\n",
      "491 0.00013523231609724462\n",
      "492 0.00013262906577438116\n",
      "493 0.00013034937728662044\n",
      "494 0.00012780439283233136\n",
      "495 0.00012565820361487567\n",
      "496 0.00012397213140502572\n",
      "497 0.00012204864469822496\n",
      "498 0.00011973935033893213\n",
      "499 0.00011752837599487975\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "# \n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T02:00:08.114528800Z",
     "start_time": "2024-02-15T02:00:07.515221200Z"
    }
   },
   "id": "6bc540821d39dd99",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27082540.0\n",
      "1 22889330.0\n",
      "2 25783054.0\n",
      "3 32690892.0\n",
      "4 39422956.0\n",
      "5 39447416.0\n",
      "6 29799432.0\n",
      "7 16613492.0\n",
      "8 7460503.5\n",
      "9 3255219.0\n",
      "10 1648669.0\n",
      "11 1036029.8125\n",
      "12 767065.6875\n",
      "13 620430.5\n",
      "14 522809.96875\n",
      "15 449106.9375\n",
      "16 389791.53125\n",
      "17 340534.53125\n",
      "18 298966.8125\n",
      "19 263548.4375\n",
      "20 233196.8125\n",
      "21 207026.1875\n",
      "22 184354.921875\n",
      "23 164681.34375\n",
      "24 147553.5\n",
      "25 132569.359375\n",
      "26 119413.203125\n",
      "27 107808.1875\n",
      "28 97539.046875\n",
      "29 88410.1796875\n",
      "30 80291.78125\n",
      "31 73050.03125\n",
      "32 66577.53125\n",
      "33 60778.8984375\n",
      "34 55574.48828125\n",
      "35 50890.97265625\n",
      "36 46672.75\n",
      "37 42862.77734375\n",
      "38 39419.59375\n",
      "39 36299.78125\n",
      "40 33466.234375\n",
      "41 30889.296875\n",
      "42 28541.896484375\n",
      "43 26401.43359375\n",
      "44 24445.111328125\n",
      "45 22656.09765625\n",
      "46 21017.30078125\n",
      "47 19517.955078125\n",
      "48 18139.5\n",
      "49 16871.740234375\n",
      "50 15706.25\n",
      "51 14632.45703125\n",
      "52 13642.138671875\n",
      "53 12728.08203125\n",
      "54 11882.587890625\n",
      "55 11100.2890625\n",
      "56 10376.3203125\n",
      "57 9705.353515625\n",
      "58 9082.8115234375\n",
      "59 8504.8544921875\n",
      "60 7968.03369140625\n",
      "61 7469.2958984375\n",
      "62 7005.7490234375\n",
      "63 6574.0625\n",
      "64 6171.72509765625\n",
      "65 5796.587890625\n",
      "66 5445.9775390625\n",
      "67 5115.66650390625\n",
      "68 4807.5693359375\n",
      "69 4520.12158203125\n",
      "70 4251.4140625\n",
      "71 4000.083740234375\n",
      "72 3764.93115234375\n",
      "73 3544.7529296875\n",
      "74 3338.607177734375\n",
      "75 3145.4658203125\n",
      "76 2964.3564453125\n",
      "77 2794.507568359375\n",
      "78 2635.20068359375\n",
      "79 2485.70751953125\n",
      "80 2345.357666015625\n",
      "81 2213.435546875\n",
      "82 2089.412109375\n",
      "83 1972.8343505859375\n",
      "84 1863.1690673828125\n",
      "85 1759.999755859375\n",
      "86 1662.89892578125\n",
      "87 1571.505859375\n",
      "88 1485.4564208984375\n",
      "89 1404.3978271484375\n",
      "90 1327.974853515625\n",
      "91 1255.957763671875\n",
      "92 1188.057861328125\n",
      "93 1124.0064697265625\n",
      "94 1063.642333984375\n",
      "95 1006.7158813476562\n",
      "96 952.97314453125\n",
      "97 902.2418212890625\n",
      "98 854.35205078125\n",
      "99 809.1318359375\n",
      "100 766.4202270507812\n",
      "101 726.0650634765625\n",
      "102 687.9696044921875\n",
      "103 651.91064453125\n",
      "104 617.8284912109375\n",
      "105 585.6068115234375\n",
      "106 555.135009765625\n",
      "107 526.3124389648438\n",
      "108 499.0494079589844\n",
      "109 473.2622375488281\n",
      "110 448.85260009765625\n",
      "111 425.754638671875\n",
      "112 403.88092041015625\n",
      "113 383.1741943359375\n",
      "114 363.57769775390625\n",
      "115 345.0128173828125\n",
      "116 327.42578125\n",
      "117 310.76666259765625\n",
      "118 294.9819641113281\n",
      "119 280.0257263183594\n",
      "120 265.8541259765625\n",
      "121 252.4210205078125\n",
      "122 239.68609619140625\n",
      "123 227.618408203125\n",
      "124 216.173828125\n",
      "125 205.32643127441406\n",
      "126 195.03648376464844\n",
      "127 185.2777099609375\n",
      "128 176.02391052246094\n",
      "129 167.24664306640625\n",
      "130 158.91903686523438\n",
      "131 151.01394653320312\n",
      "132 143.51486206054688\n",
      "133 136.39535522460938\n",
      "134 129.6378173828125\n",
      "135 123.22381591796875\n",
      "136 117.13566589355469\n",
      "137 111.35652160644531\n",
      "138 105.86892700195312\n",
      "139 100.6589126586914\n",
      "140 95.71226501464844\n",
      "141 91.01405334472656\n",
      "142 86.5516586303711\n",
      "143 82.31287384033203\n",
      "144 78.28739929199219\n",
      "145 74.4625015258789\n",
      "146 70.82852172851562\n",
      "147 67.37615966796875\n",
      "148 64.09549713134766\n",
      "149 60.977500915527344\n",
      "150 58.01472473144531\n",
      "151 55.198753356933594\n",
      "152 52.5225830078125\n",
      "153 49.978668212890625\n",
      "154 47.56110382080078\n",
      "155 45.26283645629883\n",
      "156 43.07725524902344\n",
      "157 40.99990463256836\n",
      "158 39.024322509765625\n",
      "159 37.146385192871094\n",
      "160 35.35970687866211\n",
      "161 33.66118621826172\n",
      "162 32.04541778564453\n",
      "163 30.510108947753906\n",
      "164 29.048547744750977\n",
      "165 27.65772247314453\n",
      "166 26.335350036621094\n",
      "167 25.07693099975586\n",
      "168 23.879776000976562\n",
      "169 22.740720748901367\n",
      "170 21.65671157836914\n",
      "171 20.625606536865234\n",
      "172 19.644237518310547\n",
      "173 18.71065902709961\n",
      "174 17.82192039489746\n",
      "175 16.976301193237305\n",
      "176 16.17139434814453\n",
      "177 15.405313491821289\n",
      "178 14.67606258392334\n",
      "179 13.981645584106445\n",
      "180 13.320915222167969\n",
      "181 12.692331314086914\n",
      "182 12.093076705932617\n",
      "183 11.522797584533691\n",
      "184 10.980135917663574\n",
      "185 10.462763786315918\n",
      "186 9.970772743225098\n",
      "187 9.501811981201172\n",
      "188 9.055481910705566\n",
      "189 8.630329132080078\n",
      "190 8.225430488586426\n",
      "191 7.840062141418457\n",
      "192 7.472694396972656\n",
      "193 7.122937202453613\n",
      "194 6.789812088012695\n",
      "195 6.4723711013793945\n",
      "196 6.170042991638184\n",
      "197 5.882073402404785\n",
      "198 5.607723712921143\n",
      "199 5.346271991729736\n",
      "200 5.097474575042725\n",
      "201 4.860176086425781\n",
      "202 4.6339850425720215\n",
      "203 4.41852331161499\n",
      "204 4.213170051574707\n",
      "205 4.017621994018555\n",
      "206 3.831165075302124\n",
      "207 3.6534032821655273\n",
      "208 3.484184503555298\n",
      "209 3.3227601051330566\n",
      "210 3.169062852859497\n",
      "211 3.022399663925171\n",
      "212 2.882657527923584\n",
      "213 2.7495243549346924\n",
      "214 2.6225790977478027\n",
      "215 2.5015313625335693\n",
      "216 2.386155605316162\n",
      "217 2.2761385440826416\n",
      "218 2.171262264251709\n",
      "219 2.071335792541504\n",
      "220 1.9760581254959106\n",
      "221 1.8851182460784912\n",
      "222 1.7984998226165771\n",
      "223 1.7159318923950195\n",
      "224 1.6371021270751953\n",
      "225 1.5620417594909668\n",
      "226 1.490456461906433\n",
      "227 1.422055721282959\n",
      "228 1.356947422027588\n",
      "229 1.2948527336120605\n",
      "230 1.23560631275177\n",
      "231 1.1790908575057983\n",
      "232 1.125267505645752\n",
      "233 1.0738215446472168\n",
      "234 1.0247821807861328\n",
      "235 0.9780054092407227\n",
      "236 0.9334343075752258\n",
      "237 0.8908936977386475\n",
      "238 0.8502858877182007\n",
      "239 0.8115420341491699\n",
      "240 0.7746404409408569\n",
      "241 0.739472508430481\n",
      "242 0.7057590484619141\n",
      "243 0.6737078428268433\n",
      "244 0.6430627107620239\n",
      "245 0.6139143705368042\n",
      "246 0.5860576629638672\n",
      "247 0.5594825744628906\n",
      "248 0.5341477394104004\n",
      "249 0.509932816028595\n",
      "250 0.48679402470588684\n",
      "251 0.4647601246833801\n",
      "252 0.443756639957428\n",
      "253 0.4236605167388916\n",
      "254 0.4045279026031494\n",
      "255 0.3862293064594269\n",
      "256 0.3687915802001953\n",
      "257 0.35215675830841064\n",
      "258 0.33624446392059326\n",
      "259 0.3210749626159668\n",
      "260 0.30660510063171387\n",
      "261 0.2927864193916321\n",
      "262 0.27958258986473083\n",
      "263 0.267020583152771\n",
      "264 0.2549591064453125\n",
      "265 0.24351853132247925\n",
      "266 0.2325746715068817\n",
      "267 0.22213585674762726\n",
      "268 0.21215343475341797\n",
      "269 0.2026309221982956\n",
      "270 0.1935391128063202\n",
      "271 0.18483147025108337\n",
      "272 0.1765434741973877\n",
      "273 0.16862733662128448\n",
      "274 0.16109591722488403\n",
      "275 0.15386328101158142\n",
      "276 0.14696946740150452\n",
      "277 0.14041468501091003\n",
      "278 0.13412317633628845\n",
      "279 0.1281334012746811\n",
      "280 0.1223912313580513\n",
      "281 0.11691053211688995\n",
      "282 0.11168202757835388\n",
      "283 0.10672412067651749\n",
      "284 0.10197073221206665\n",
      "285 0.09740366041660309\n",
      "286 0.09305611252784729\n",
      "287 0.08894264698028564\n",
      "288 0.08496901392936707\n",
      "289 0.08118538558483124\n",
      "290 0.07757621258497238\n",
      "291 0.07411074638366699\n",
      "292 0.07083353400230408\n",
      "293 0.06768486648797989\n",
      "294 0.06467452645301819\n",
      "295 0.061812806874513626\n",
      "296 0.05906381085515022\n",
      "297 0.05644511431455612\n",
      "298 0.05394691228866577\n",
      "299 0.0515645295381546\n",
      "300 0.04926656559109688\n",
      "301 0.047086723148822784\n",
      "302 0.04500914365053177\n",
      "303 0.04302787035703659\n",
      "304 0.041120048612356186\n",
      "305 0.03930557519197464\n",
      "306 0.037565477192401886\n",
      "307 0.035921353846788406\n",
      "308 0.03433307632803917\n",
      "309 0.03281918913125992\n",
      "310 0.03137447312474251\n",
      "311 0.02998627908527851\n",
      "312 0.02868693694472313\n",
      "313 0.027417510747909546\n",
      "314 0.026212060824036598\n",
      "315 0.025065667927265167\n",
      "316 0.023969147354364395\n",
      "317 0.022921932861208916\n",
      "318 0.02191643975675106\n",
      "319 0.020960653200745583\n",
      "320 0.0200539268553257\n",
      "321 0.019177217036485672\n",
      "322 0.01834132894873619\n",
      "323 0.017535967752337456\n",
      "324 0.016771957278251648\n",
      "325 0.016043001785874367\n",
      "326 0.015347735956311226\n",
      "327 0.014682786539196968\n",
      "328 0.014038126915693283\n",
      "329 0.013438638299703598\n",
      "330 0.012855243869125843\n",
      "331 0.012308831326663494\n",
      "332 0.011783935129642487\n",
      "333 0.011271895840764046\n",
      "334 0.010786183178424835\n",
      "335 0.01032387837767601\n",
      "336 0.009886110201478004\n",
      "337 0.009463762864470482\n",
      "338 0.009060479700565338\n",
      "339 0.008676858618855476\n",
      "340 0.008310621604323387\n",
      "341 0.007958593778312206\n",
      "342 0.007623632438480854\n",
      "343 0.007305734790861607\n",
      "344 0.006999795325100422\n",
      "345 0.006704980041831732\n",
      "346 0.006426836363971233\n",
      "347 0.006157365627586842\n",
      "348 0.005902350880205631\n",
      "349 0.005655709188431501\n",
      "350 0.005426002666354179\n",
      "351 0.005203059874475002\n",
      "352 0.00498984707519412\n",
      "353 0.004782922565937042\n",
      "354 0.004584955982863903\n",
      "355 0.004402046091854572\n",
      "356 0.004227266646921635\n",
      "357 0.004056702367961407\n",
      "358 0.0038926685228943825\n",
      "359 0.0037351478822529316\n",
      "360 0.0035875970497727394\n",
      "361 0.0034451675601303577\n",
      "362 0.0033086712937802076\n",
      "363 0.00318088848143816\n",
      "364 0.0030578149016946554\n",
      "365 0.0029359874315559864\n",
      "366 0.0028197751380503178\n",
      "367 0.0027116613928228617\n",
      "368 0.0026091663166880608\n",
      "369 0.002507901517674327\n",
      "370 0.0024112239480018616\n",
      "371 0.0023199187126010656\n",
      "372 0.002233132952824235\n",
      "373 0.002149130217730999\n",
      "374 0.002066117711365223\n",
      "375 0.0019899988546967506\n",
      "376 0.0019197061192244291\n",
      "377 0.001848442479968071\n",
      "378 0.0017785511445254087\n",
      "379 0.001715891994535923\n",
      "380 0.0016546646365895867\n",
      "381 0.0015952130779623985\n",
      "382 0.001537919626571238\n",
      "383 0.0014834341127425432\n",
      "384 0.0014320207992568612\n",
      "385 0.0013828250812366605\n",
      "386 0.0013336394913494587\n",
      "387 0.001288615632802248\n",
      "388 0.0012439676793292165\n",
      "389 0.0012011842336505651\n",
      "390 0.0011626211926341057\n",
      "391 0.0011229034280404449\n",
      "392 0.001085585681721568\n",
      "393 0.00105084921233356\n",
      "394 0.0010172411566600204\n",
      "395 0.000983579084277153\n",
      "396 0.0009523326298221946\n",
      "397 0.000921976170502603\n",
      "398 0.0008918974781408906\n",
      "399 0.0008626979542896152\n",
      "400 0.0008366094552911818\n",
      "401 0.0008099238038994372\n",
      "402 0.0007850585971027613\n",
      "403 0.0007606693543493748\n",
      "404 0.0007389882812276483\n",
      "405 0.000715347588993609\n",
      "406 0.0006938339793123305\n",
      "407 0.0006731668254360557\n",
      "408 0.0006541791372001171\n",
      "409 0.0006349572213366628\n",
      "410 0.0006148229585960507\n",
      "411 0.0005977002438157797\n",
      "412 0.00058006338076666\n",
      "413 0.0005640945746563375\n",
      "414 0.0005474230274558067\n",
      "415 0.0005321358912624419\n",
      "416 0.000517447420861572\n",
      "417 0.0005038148956373334\n",
      "418 0.0004891387652605772\n",
      "419 0.00047568604350090027\n",
      "420 0.0004628835304174572\n",
      "421 0.0004499254282563925\n",
      "422 0.0004375735588837415\n",
      "423 0.0004262394504621625\n",
      "424 0.0004150602617301047\n",
      "425 0.0004031999269500375\n",
      "426 0.00039238709723576903\n",
      "427 0.0003824549785349518\n",
      "428 0.0003732798504643142\n",
      "429 0.0003635765169747174\n",
      "430 0.00035431073047220707\n",
      "431 0.0003459847066551447\n",
      "432 0.0003369877813383937\n",
      "433 0.0003279722295701504\n",
      "434 0.00031997053883969784\n",
      "435 0.0003126748197246343\n",
      "436 0.0003042729222215712\n",
      "437 0.0002964147424791008\n",
      "438 0.0002893841592594981\n",
      "439 0.0002824478433467448\n",
      "440 0.000276283361017704\n",
      "441 0.00026954099303111434\n",
      "442 0.00026287735090591013\n",
      "443 0.00025643911794759333\n",
      "444 0.00025085214292630553\n",
      "445 0.00024521531304344535\n",
      "446 0.00023931151372380555\n",
      "447 0.0002339120692340657\n",
      "448 0.00022842548787593842\n",
      "449 0.000223615177674219\n",
      "450 0.00021835335064679384\n",
      "451 0.00021403760183602571\n",
      "452 0.00020935115753673017\n",
      "453 0.0002050131733994931\n",
      "454 0.00020063447300344706\n",
      "455 0.000196279666852206\n",
      "456 0.00019206185243092477\n",
      "457 0.0001881571370176971\n",
      "458 0.00018416291277389973\n",
      "459 0.00018038699636235833\n",
      "460 0.00017632324306759983\n",
      "461 0.00017244488117285073\n",
      "462 0.00016936921747401357\n",
      "463 0.00016588409198448062\n",
      "464 0.0001623470161575824\n",
      "465 0.00015948567306622863\n",
      "466 0.00015615765005350113\n",
      "467 0.00015346499276347458\n",
      "468 0.00015008539776317775\n",
      "469 0.00014719177852384746\n",
      "470 0.0001444396621081978\n",
      "471 0.00014155817916616797\n",
      "472 0.000138802919536829\n",
      "473 0.00013603875413537025\n",
      "474 0.00013343726459424943\n",
      "475 0.00013094076712150127\n",
      "476 0.00012851381325162947\n",
      "477 0.00012627424439415336\n",
      "478 0.00012382811110001057\n",
      "479 0.00012228067498654127\n",
      "480 0.00011991654173471034\n",
      "481 0.00011747798271244392\n",
      "482 0.00011526208254508674\n",
      "483 0.0001131855824496597\n",
      "484 0.00011154291860293597\n",
      "485 0.00010943883535219356\n",
      "486 0.0001075758264050819\n",
      "487 0.00010568591096671298\n",
      "488 0.00010377971193520352\n",
      "489 0.00010232915519736707\n",
      "490 0.00010038446635007858\n",
      "491 9.880219295155257e-05\n",
      "492 9.699985821498558e-05\n",
      "493 9.561003389535472e-05\n",
      "494 9.365567530039698e-05\n",
      "495 9.248543938156217e-05\n",
      "496 9.119442256633192e-05\n",
      "497 8.934770448831841e-05\n",
      "498 8.795362373348325e-05\n",
      "499 8.676249126438051e-05\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_custom_function.py\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a context object and a Tensor containing the\n",
    "    input; we must return a Tensor containing the output, and we can use the\n",
    "    context object to cache objects for use in the backward pass.\n",
    "    \"\"\"\n",
    "    ctx.save_for_backward(x)\n",
    "    return x.clamp(min=0)\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive the context object and a Tensor containing\n",
    "    the gradient of the loss with respect to the output produced during the\n",
    "    forward pass. We can retrieve cached data from the context object, and must\n",
    "    compute and return the gradient of the loss with respect to the input to the\n",
    "    forward function.\n",
    "    \"\"\"\n",
    "    x, = ctx.saved_tensors\n",
    "    grad_x = grad_output.clone()\n",
    "    grad_x[x < 0] = 0\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "  # custom ReLU implementation using the MyReLU.apply function\n",
    "  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    " \n",
    "  # Compute and print loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass.\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Update weights using gradient descent\n",
    "    w1 - learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T02:43:43.114600900Z",
     "start_time": "2024-02-15T02:43:42.566142400Z"
    }
   },
   "id": "bb417ab9727f1bcc",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\96446\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 13\u001B[0m\n\u001B[0;32m      9\u001B[0m N, D_in, H, D_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Create placeholders for the input and target data; these will be filled\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# with real data when we execute the graph.\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m x \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mplaceholder(tf\u001B[38;5;241m.\u001B[39mfloat32, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, D_in))\n\u001B[0;32m     14\u001B[0m y \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mplaceholder(tf\u001B[38;5;241m.\u001B[39mfloat32, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, D_out))\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Create Variables for the weights and initialize them with random data.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# A TensorFlow Variable persists its value across executions of the graph.\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:05:43.802815Z",
     "start_time": "2024-02-15T03:05:38.654154900Z"
    }
   },
   "id": "bf3090eebd647afd",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "275db6205fc1ec29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
